import json
import scripts.util as util
import scripts.storage as storage
import scripts.database as database
import scripts.git_providers as git_providers
import scripts.parse_git_commits as parse_git_commits
import scripts.commit_size as commit_size

configfile: "config.yaml"

printlog = util.setup_logger()

GIT_SEARCH_START_DATE_STR = config["search_start_date"]
GIT_SEARCH_START_DATE     = util.str_to_date(GIT_SEARCH_START_DATE_STR)
GIT_SEARCH_END_DATE_STR   = config["search_end_date"]
GIT_SEARCH_END_DATE       = util.str_to_date(GIT_SEARCH_END_DATE_STR)
GIT_SEARCH_INTERVAL       = config["search_interval"]
GIT_SEARCH_QUERY          = config["search_query"]
GIT_PROVIDER              = config["git_provider"]
GIT_PROVIDER_TOKEN        = config["git_provider_token"]
DB_ENGINE                 = config["database"]["engine"]
DB_CONFIG                 = config["database_config"][DB_ENGINE]
BLACKLIST                 = config["snakemake-catalog-urls"]["blacklist"]
SKIPS                     = config["snakemake-catalog-urls"]["skips"]

WORKDIR = config["result_dir"] + f"/{GIT_PROVIDER}_{GIT_SEARCH_START_DATE_STR}_{GIT_SEARCH_END_DATE_STR}"
CLONEDIR = config["result_dir"] + f"/{GIT_PROVIDER}_{GIT_SEARCH_START_DATE_STR}_{GIT_SEARCH_END_DATE_STR}/cloned_repositories"
NOW     = util.now()

storage_handler = storage.Storage(logger=printlog, engine="file", storage_config={"is_absolute": True})

git_handler = git_providers.GitProvider(
    provider=GIT_PROVIDER,
    token=GIT_PROVIDER_TOKEN,
    logger=printlog)

search_date_ranges = util.generate_date_ranges(GIT_SEARCH_START_DATE, GIT_SEARCH_END_DATE, GIT_SEARCH_INTERVAL)
START_DATES = [date_range[0] for date_range in search_date_ranges]
END_DATES   = [date_range[1] for date_range in search_date_ranges]
printlog.debug(f"search start dates: {START_DATES}")
printlog.debug(f"search end dates: {END_DATES}")


rule commit_size_analysis:
    input:
        WORKDIR + "/.done_clone_repositories"
    params:
        cloned_repos_dir = WORKDIR + "/cloned_repositories"
    output:
        WORKDIR + "/.done_commit_size_analysis"
    run:
        commit_size.process_all_repositories(params.cloned_repos_dir)
        storage_handler.write(output[0], "")


rule migrate_jsonfiles_to_database:
    input:
        WORKDIR + "/.done_clone_repositories",
        WORKDIR + "/cloned_repositories.txt",
        WORKDIR + "/repositories_to_clone.txt",
        WORKDIR + "/.done_get_issues",
        WORKDIR + "/.done_analyse_git_commit"

    params:
        json_files = expand(WORKDIR + "/search_query_result_{start_date}_{end_date}.json", zip, start_date=START_DATES, end_date=END_DATES),
        skiplist_file = WORKDIR + "/skiplist_all.txt"

    output:
        WORKDIR + "/.done_migrate_jsonfiles_to_database",
        WORKDIR + "/migrated_repositories.txt"

    run:
        cloned_repositories = storage_handler.read(input[1], multiple_lines=True)
        repositories_to_clone = storage_handler.read(input[2], multiple_lines=True)

        for repo in repositories_to_clone:
            if repo not in cloned_repositories:
                printlog.warning("Repository not cloned: " + repo)

        db = database.Database(logger=printlog, engine=DB_ENGINE,
                                git_provider=GIT_PROVIDER, db_config=DB_CONFIG)

        stored_repositories = []
        for json_file in params.json_files:
            api_response = storage_handler.read(json_file, from_json=True)
            if "items" in api_response and len(api_response["items"]) > 0:
                stored_repositories = db.store_search_repositories_results(
                                        api_response, cloned_repositories)
            else:
                printlog.debug(f"No items in {json_file}")

        db.close()

        storage_handler.write(output[1], stored_repositories)
        storage_handler.write(output[0], "")


rule analyse_git_commit:
    input:
        WORKDIR + "/.done_clone_repositories",
        WORKDIR + "/cloned_repositories.txt",
        WORKDIR + "/repositories_to_clone.txt"

    params:
        cloned_repos_dir = WORKDIR + "/cloned_repositories"

    output:
        WORKDIR + "/metadata_git_commit.txt",
        WORKDIR + "/.done_analyse_git_commit"

    run:
        parse_git_commits.process_all_repositories(params.cloned_repos_dir, output[0])

        storage_handler.write(output[1], "")


rule get_issues:
    input:
        WORKDIR + "/.done_clone_repositories",
        WORKDIR + "/cloned_repositories.txt",
        WORKDIR + "/repositories_to_clone.txt"

    params:
        search_result_files = expand(WORKDIR + "/search_query_result_{start_date}_{end_date}.json", zip, start_date=START_DATES, end_date=END_DATES),
        issues_dir = WORKDIR + "/issues_by_repository"

    output:
        WORKDIR + "/.done_get_issues",
        WORKDIR + "/repositories_with_issues.txt"

    run:
        cloned_repositories = []
        issues_repositories = []
        full_names = storage_handler.read(input[1], multiple_lines=True)
        for full_name in full_names:
            owner, repo = full_name.split("/")
            cloned_repositories.append({"owner": owner, "repo": repo})

        for search_result_file in params.search_result_files:
            search_api_response = storage_handler.read(search_result_file, from_json=True)
            if "items" not in search_api_response or len(search_api_response["items"]) < 1:
                printlog.error(f"No items in {search_result_file}")
                continue

            for repo_details in search_api_response["items"]:
                owner, repo = repo_details["full_name"].split("/")
                if {"owner": owner, "repo": repo} not in cloned_repositories:
                    continue

                if "has_issues" not in repo_details or not repo_details["has_issues"]:
                    printlog.debug("Issues is disabled for %s" % repo_details["full_name"])
                    continue

                issues = git_handler.get_issues(owner, repo)

                if issues["total_count"] < 1:
                    printlog.debug(f"No issues in {owner}/{repo}")
                    continue

                storage_handler.write(f"{params.issues_dir}/{owner}_{repo}_issues.json", issues["items"])
                printlog.info(f"Saved issues for {owner}/{repo}, total: {issues['total_count']}")
                issues_repositories.append(f"{owner}/{repo}")

        storage_handler.write(output[1], issues_repositories)
        storage_handler.write(output[0], "")


rule clone_repositories:
    input:
        WORKDIR + "/.done_download_skiplist_from_catalog",
        WORKDIR + "/skiplist_all.txt"

    params:
        repolist_file = WORKDIR + "/repository_list_from_search.txt"

    output:
        WORKDIR + "/cloned_repositories.txt",
        WORKDIR + "/repositories_to_clone.txt",
        WORKDIR + "/.done_clone_repositories"

    run:
        skiplist_all = storage_handler.read(input[1], multiple_lines=True)

        skip_count = 0
        target_count = 0
        repositories_to_clone = storage_handler.read(params.repolist_file, multiple_lines=True)
        for repo in repositories_to_clone:
            if repo in skiplist_all:
                skip_count += 1
                repositories_to_clone.remove(repo)
            else:
                target_count += 1

        printlog.info("Number of repositories to clone: %s (skipped: %s)" %
                                                (target_count, skip_count))

        storage_handler.write(output[1], repositories_to_clone)

        cloned_repositories = git_handler.clone_repositories(CLONEDIR,
                                repositories_to_clone)

        printlog.info("Number of cloned repositories: %s" % len(cloned_repositories))

        storage_handler.write(output[0], cloned_repositories)
        storage_handler.write(output[2], "")


rule download_skiplist_from_catalog:
    input:
        WORKDIR + "/.done_search_repositories",

    params:
        file_skiplist  = WORKDIR + "/skips.json",
        file_blacklist = WORKDIR + "/blacklist.txt",

    output:
        WORKDIR + "/skips.json",
        WORKDIR + "/blacklist.txt",
        WORKDIR + "/skiplist_all.txt",
        WORKDIR + "/.done_download_skiplist_from_catalog"
    run:
        util.download_http_file(SKIPS, output[0])
        util.download_http_file(BLACKLIST, output[1])

        merged_skiplist = util.merge_skiplist(file_blacklist=output[1],
                          file_skiplist=output[0])

        storage_handler.write(output[2], merged_skiplist)
        storage_handler.write(output[3], "")


rule search_repositories:
    input:
        WORKDIR + "/search_queries.txt"

    params:
        json_files = expand(WORKDIR + "/search_query_result_{start_date}_{end_date}.json", zip, start_date=START_DATES, end_date=END_DATES)

    output:
        WORKDIR + "/.done_search_repositories",
        WORKDIR + "/repository_list_from_search.txt",
        expand(WORKDIR + "/search_query_result_{start_date}_{end_date}.json", zip, start_date=START_DATES, end_date=END_DATES)

    run:
        json_data = None

        query_strings = storage_handler.read(input[0], multiple_lines=True)
        for query_string in query_strings:
            date_range  = query_string.split("created:")[1].split("..")
            start_date  = date_range[0]
            end_date    = date_range[1]
            output_file = f"{WORKDIR}/search_query_result_{start_date}_{end_date}.json"

            # If the output file exists, this is a case that the rule was
            # stopped in the middle of the execution. Existing output file
            # means that the search for the query was already done.
            # Accordingly, we skip this query.
            if os.path.exists(output_file):
                continue

            result = git_handler.search_repositories(query=query_string)
            storage_handler.write(output_file, result, to_json=True)

        items = []
        # create repository_list_from_search.txt
        for json_file in params.json_files:
            json_data = storage_handler.read(json_file, from_json=True)
            for item in json_data["items"]:
                items.append(item["full_name"])

        storage_handler.write(output[1], items)
        storage_handler.write(output[0], "")


rule generate_search_queries:
    input:
        WORKDIR + "/.initialized"
    output:
        WORKDIR + "/search_queries.txt"
    params:
        dates=search_date_ranges
    run:
        query_strings = []
        for date_range in params.dates:
            # snakemake workflow in:readme archived:false created:2024-11-01..2024-12-31
            query_string = f"{GIT_SEARCH_QUERY} created:{date_range[0]}..{date_range[1]}"
            query_strings.append(query_string)

        storage_handler.write(output[0], query_strings)

rule make_work_directory:
    """ Create a work directory as ./results/{GIT_PROVIDER}_{GIT_SEARCH_START_DATE}_{GIT_SEARCH_END_DATE} """
    output:
        WORKDIR + "/.initialized"
    run:
        os.makedirs(f"results/{GIT_PROVIDER}_{GIT_SEARCH_START_DATE}_{GIT_SEARCH_END_DATE}", exist_ok=True)
        # create .initialized file in the directory.
        storage_handler.write(output[0], "")
