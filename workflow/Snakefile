import json
import scripts.util as util
import scripts.database as database
import scripts.git_providers as git_providers

configfile: "config.yaml"

printlog = util.setup_logger()

GIT_SEARCH_START_DATE_STR = config["github_search_start_date"]
GIT_SEARCH_START_DATE     = util.str_to_date(GIT_SEARCH_START_DATE_STR)
GIT_SEARCH_END_DATE_STR   = config["github_search_end_date"]
GIT_SEARCH_END_DATE       = util.str_to_date(GIT_SEARCH_END_DATE_STR)
GIT_SEARCH_INTERVAL       = config["github_search_interval"]
GIT_SEARCH_QUERY          = config["github_search_query"]
WORKDIR = config["result_dir"] + f"/{GIT_SEARCH_START_DATE_STR}_{GIT_SEARCH_END_DATE_STR}"
NOW     = util.now()

github_search_date_ranges = util.generate_date_ranges(GIT_SEARCH_START_DATE, GIT_SEARCH_END_DATE, GIT_SEARCH_INTERVAL)
START_DATES = [date_range[0] for date_range in github_search_date_ranges]
END_DATES   = [date_range[1] for date_range in github_search_date_ranges]
printlog.debug(f"GitHub search start dates: {START_DATES}")
printlog.debug(f"GitHub search end dates: {END_DATES}")


rule store_metadata_in_database:
    input:
        expand(WORKDIR + "/github_search_query_result_{start_date}_{end_date}.json", zip, start_date=START_DATES, end_date=END_DATES)
    output:
        WORKDIR + "/.done_storing_metadata"
    run:
        db = database.Database(engine=config["database"]["engine"], logger=printlog)
        for input_file in input:
            with open(input_file, 'r') as f:
                json_data = json.load(f)
                db.store_github_repository_search_results(json_data)
        db.close()

        # create .done_storing_metadata file
        with open(output[0], 'w') as f:
            f.write("")


rule search_github_repositories:
    input:
        WORKDIR + "/github_search_queries.txt"
    output:
        WORKDIR + "/.done_search_github_repositories"
    run:
        json_data = None
        git_provider = git_providers.GitProvider(
                provider=config["git_provider"],
                token=config["git_provider_token"],
                logger=printlog)

        with open(input[0], 'r') as f:
            for line in f:
                query_string = line.strip()
                date_range  = query_string.split("created:")[1].split("..")
                start_date  = date_range[0]
                end_date    = date_range[1]
                output_file = f"{WORKDIR}/github_search_query_result_{start_date}_{end_date}.json"

                # If the output file exists, this is a case that the rule was
                # stopped in the middle of the execution. Existing output file
                # means that the search for the query was already done.
                # Accordingly, we skip this query.
                if os.path.exists(output_file):
                    continue

                result = git_provider.search_repositories(query=query_string)

                with open(output_file, 'w') as f:
                    json.dump(result, f, indent=4)

        # create .done_search_github_repositories file
        with open(output[0], 'w') as f:
            f.write("")


rule generate_github_search_queries:
    input:
        WORKDIR + "/.initialized"
    output:
        WORKDIR + "/github_search_queries.txt"
    params:
        dates=github_search_date_ranges
    run:
        with open(output[0], 'w') as f:
            for date_range in params.dates:
                # snakemake workflow in:readme archived:false created:2024-11-01..2024-12-31
                query_string = f"{GIT_SEARCH_QUERY} created:{date_range[0]}..{date_range[1]}"
                f.write(f"{query_string}\n")


rule make_work_directory:
    """ make a work directory as ./results/{START_DATE}_{END_DATE} """
    output:
        WORKDIR + "/.initialized"
    run:
        os.makedirs(f"results/{GIT_SEARCH_START_DATE}_{GIT_SEARCH_END_DATE}", exist_ok=True)
        # create .initialized file in the directory.
        with open(output[0], 'w') as f:
            f.write("")
