import os
import scripts.util as util
import scripts.storage as storage
import scripts.database as database
import scripts.git_providers as git_providers
import scripts.parse_git_commits as parse_git_commits
import scripts.commit_size as commit_size

configfile: "config.yaml"

printlog = util.setup_logger()

GIT_SEARCH_START_DATE_STR = config["search_start_date"]
GIT_SEARCH_START_DATE     = util.str_to_date(GIT_SEARCH_START_DATE_STR)
GIT_SEARCH_END_DATE_STR   = config["search_end_date"]
GIT_SEARCH_END_DATE       = util.str_to_date(GIT_SEARCH_END_DATE_STR)
GIT_SEARCH_INTERVAL       = config["search_interval"]
GIT_SEARCH_QUERY          = config["search_query"]
GIT_PROVIDER              = config["git_provider"]
GIT_PROVIDER_TOKEN        = config["git_provider_token"]
DB_ENGINE                 = config["database"]["engine"]
DB_CONFIG                 = config["database_config"][DB_ENGINE]
BLACKLIST                 = config["snakemake-catalog-urls"]["blacklist"]
SKIPS                     = config["snakemake-catalog-urls"]["skips"]

WORKDIR = config["result_dir"] + f"/{GIT_PROVIDER}_{GIT_SEARCH_START_DATE_STR}_{GIT_SEARCH_END_DATE_STR}"
CLONEDIR = config["result_dir"] + f"/{GIT_PROVIDER}_{GIT_SEARCH_START_DATE_STR}_{GIT_SEARCH_END_DATE_STR}/cloned_repositories"
NOW     = util.now()

storage_handler = storage.Storage(logger=printlog, engine="file", storage_config={"is_absolute": True, "mkdir_ok": True})

git_handler = git_providers.GitProvider(
    provider=GIT_PROVIDER,
    token=GIT_PROVIDER_TOKEN,
    logger=printlog)

search_date_ranges = util.generate_date_ranges(GIT_SEARCH_START_DATE, GIT_SEARCH_END_DATE, GIT_SEARCH_INTERVAL)
START_DATES = [date_range[0] for date_range in search_date_ranges]
END_DATES   = [date_range[1] for date_range in search_date_ranges]
printlog.debug(f"search start dates: {START_DATES}")
printlog.debug(f"search end dates: {END_DATES}")


rule migrate_jsonfiles_to_database:
    input:
        WORKDIR + "/.done_clone_repositories",
        WORKDIR + "/cloned_repositories.txt",
        WORKDIR + "/repositories_to_clone.txt",
        WORKDIR + "/.done_get_issues",
        WORKDIR + "/.done_get_issues_extras",
        WORKDIR + "/.done_analyse_git_commit",
        WORKDIR + "/.done_analyse_commit_size",

    params:
        json_files = expand(WORKDIR + "/search_query_result_{start_date}_{end_date}.json", zip, start_date=START_DATES, end_date=END_DATES),
        skiplist_file = WORKDIR + "/skiplist_all.txt"

    output:
        WORKDIR + "/.done_migrate_jsonfiles_to_database",
        WORKDIR + "/migrated_repositories.txt"

    run:
        cloned_repositories = storage_handler.read(input[1], multiple_lines=True)
        repositories_to_clone = storage_handler.read(input[2], multiple_lines=True)

        for repo in repositories_to_clone:
            if repo not in cloned_repositories:
                printlog.warning("Repository not cloned: %s", repo)

        db = database.Database(logger=printlog, engine=DB_ENGINE,
                                git_provider=GIT_PROVIDER, db_config=DB_CONFIG)

        stored_repositories = []
        for json_file in params.json_files:
            api_response = storage_handler.read(json_file, from_json=True)
            if "items" in api_response and len(api_response["items"]) > 0:
                stored_repositories = db.store_search_repositories_results(
                                        api_response, cloned_repositories)
            else:
                printlog.debug("No items in %s", json_file)

        db.close()

        storage_handler.write(output[1], stored_repositories)
        storage_handler.write(output[0], "")


rule analyse_commit_size:
   input:
       WORKDIR + "/.done_clone_repositories"
   params:
       cloned_repos_dir = WORKDIR + "/cloned_repositories"
   output:
       WORKDIR + "/.done_analyse_commit_size"
   run:
       
       commit_sizes = commit_size.process_all_repositories(params.cloned_repos_dir)

       for repo_name in os.listdir(params.cloned_repos_dir):
           repo_path = os.path.join(params.cloned_repos_dir, repo_name)
           commit_sizes_file = os.path.join(repo_path, "commit_sizes.json")
           plot_file = os.path.join(repo_path, "codebase_growth.png")
           storage_handler.write(commit_sizes_file, commit_sizes[repo_name], to_json=True)
           printlog.info(f"commit_sizes.json created in {repo_path}")
           printlog.info(f"Plot saved as {plot_file}")
          
       storage_handler.write(output[0], "")


rule analyse_git_commit:
    input:
        WORKDIR + "/.done_clone_repositories",
        WORKDIR + "/cloned_repositories.txt",
        WORKDIR + "/repositories_to_clone.txt"

    params:
        cloned_repos_dir = WORKDIR + "/cloned_repositories"

    output:
        WORKDIR + "/metadata_git_commit.txt",
        WORKDIR + "/.done_analyse_git_commit"

    run:
        parse_git_commits.process_all_repositories(params.cloned_repos_dir, output[0])

        storage_handler.write(output[1], "")


rule get_issues_extras:
    input:
        WORKDIR + "/.done_get_issues",
        WORKDIR + "/repositories_with_issues.txt"

    params:
        issues_dir = WORKDIR + "/issues_by_repository"

    output:
        WORKDIR + "/.done_get_issues_extras",

    run:
        repositories_with_issues = storage_handler.read(input[1], multiple_lines=True)

        for i, repo_fullname in enumerate(repositories_with_issues):
            owner, repo = repo_fullname.strip().split("/")
            all_issues           = None
            all_comments         = list()
            all_events           = list()
            issues_non_pr        = list()
            issues_as_pr         = list()
            issues_with_comments = list()
            all_pullrequests     = list()
            all_events           = list()
            file_issues          = f"{params.issues_dir}/{owner}_{repo}_issues.json"
            file_comments        = f"{params.issues_dir}/{owner}_{repo}_issues_comments.json"
            file_pullrequests    = f"{params.issues_dir}/{owner}_{repo}_issues_pullrequests.json"
            file_events          = f"{params.issues_dir}/{owner}_{repo}_issues_events.json"

            all_issues = storage_handler.read(file_issues, from_json=True)
            if all_issues is None:
                printlog.error("Failed to read %s.", file_issues)
                continue

            for issue in all_issues:
                if "comments" in issue and issue["comments"] > 0:
                    issues_with_comments.append(issue)

                if "pull_request" in issue:
                    issues_as_pr.append(issue)
                else:
                    issues_non_pr.append(issue)

            printlog.info("Getting extras from %d issues and %d pull requests from %s.",
                            len(issues_non_pr), len(issues_as_pr), repo_fullname)

            if os.path.exists(file_pullrequests):
                printlog.info("Pull requests are already saved for %s.", repo_fullname)
            else:
                for pullrequest in issues_as_pr:
                    issue_number = pullrequest["number"]
                    details = None
                    try:
                        details = git_handler.get_pullrequest_details(owner, repo, issue_number)
                    except Exception as e:
                        printlog.error("Failed to get pull request details for %s/%s#%d: %s",
                                        owner, repo, issue_number, e)
                    if details is not None:
                        all_pullrequests.append(details)

                if len(all_pullrequests) > 0:
                    storage_handler.write(file_pullrequests, all_pullrequests, to_json=True)
                    printlog.info("Saved %d pull request details for %s",
                                    len(all_pullrequests), repo_fullname)

            if os.path.exists(file_comments):
                printlog.info("Comments are already saved for %s.", repo_fullname)
            else:
                for issue in issues_with_comments:
                    issue_number = issue["number"]
                    comments = None
                    try:
                        comments = git_handler.get_issue_comments(owner, repo, issue_number)
                    except Exception as e:
                        printlog.error("Failed to get comments for %s/%s#%d: %s",
                                        owner, repo, issue_number, e)
                    if comments is not None and len(comments) > 0:
                        all_comments.extend(comments)
                    else:
                        printlog.warning("Failed to get comments for %s/%s#%d",
                                         owner, repo, issue_number)

                if len(all_comments) > 0:
                    storage_handler.write(file_comments, all_comments, to_json=True)
                    printlog.info("Saved %d comments of %d issues for %s",
                                    len(all_comments), len(issues_with_comments), repo_fullname)
                else:
                    printlog.debug("No comments found for %s", repo_fullname)

            if os.path.exists(file_events):
                printlog.info("Events are already saved for %s.", repo_fullname)
            else:
                for issue in all_issues:
                    issue_number = issue["number"]
                    events = None
                    try:
                        events = git_handler.get_issue_events(owner, repo, issue_number)
                    except Exception as e:
                        printlog.error("Failed to get events for %s/%s#%d: %s",
                                        owner, repo, issue_number, e)
                    if events is not None and len(events) > 0:
                        all_events.extend(events)
                    else:
                        printlog.warning("Failed to get events for %s/%s#%d",
                                         owner, repo, issue_number)

                if len(all_events) > 0:
                    storage_handler.write(file_events, all_events, to_json=True)
                    printlog.info("Saved %d events of %d issues for %s",
                                    len(all_events), len(all_issues), repo_fullname)
                else:
                    printlog.debug("No events found for %s", repo_fullname)


        storage_handler.write(output[0], "")


rule get_issues:
    input:
        WORKDIR + "/.done_clone_repositories",
        WORKDIR + "/cloned_repositories.txt",
        WORKDIR + "/repositories_to_clone.txt"

    params:
        search_result_files = expand(WORKDIR + "/search_query_result_{start_date}_{end_date}.json", zip, start_date=START_DATES, end_date=END_DATES),
        issues_dir = WORKDIR + "/issues_by_repository"

    output:
        WORKDIR + "/.done_get_issues",
        WORKDIR + "/repositories_with_issues.txt"

    run:
        cloned_repositories = []
        issues_repositories = []
        full_names = storage_handler.read(input[1], multiple_lines=True)
        for full_name in full_names:
            owner, repo = full_name.split("/")
            cloned_repositories.append({"owner": owner, "repo": repo})

        for search_result_file in params.search_result_files:
            search_api_response = storage_handler.read(search_result_file, from_json=True)
            if "items" not in search_api_response or len(search_api_response["items"]) < 1:
                printlog.error("No items in %s.", search_result_file)
                continue

            for repo_details in search_api_response["items"]:
                full_name = repo_details["full_name"]
                owner, repo = full_name.split("/")

                if os.path.exists(f"{params.issues_dir}/{owner}_{repo}_issues.json"):
                    printlog.info("Issues are already saved for %s.", full_name)
                    issues_repositories.append(full_name)
                    continue

                if {"owner": owner, "repo": repo} not in cloned_repositories:
                    continue

                if "has_issues" not in repo_details or not repo_details["has_issues"]:
                    printlog.debug("Issues is disabled for %s", full_name)
                    continue

                issues = None
                try:
                    issues = git_handler.get_issues(owner, repo)
                except Exception as e:
                    printlog.error("Failed to get issues for %s: %s", full_name, e)

                if issues is None or issues["total_count"] < 1:
                    printlog.debug("No issues in %s.", full_name)
                    continue

                storage_handler.write(f"{params.issues_dir}/{owner}_{repo}_issues.json", issues["items"], to_json=True)
                printlog.info("Saved issues for %s, total: %d", full_name, issues["total_count"])
                issues_repositories.append(f"{owner}/{repo}")

        storage_handler.write(output[1], issues_repositories)
        storage_handler.write(output[0], "")


rule clone_repositories:
    input:
        WORKDIR + "/.done_download_skiplist_from_catalog",
        WORKDIR + "/skiplist_all.txt"

    params:
        repolist_file = WORKDIR + "/repository_list_from_search.txt"

    output:
        WORKDIR + "/cloned_repositories.txt",
        WORKDIR + "/repositories_to_clone.txt",
        WORKDIR + "/.done_clone_repositories"

    run:
        skiplist_all = storage_handler.read(input[1], multiple_lines=True)

        skip_count = 0
        target_count = 0
        repositories_to_clone = []
        searched_repositories = storage_handler.read(params.repolist_file, multiple_lines=True)
        for repo in searched_repositories:
            if repo in skiplist_all:
                skip_count += 1
            else:
                repositories_to_clone.append(repo)
                target_count += 1

        printlog.info("Number of repositories to clone: %s (skipped: %s)",
                                                target_count, skip_count)

        storage_handler.write(output[1], repositories_to_clone)

        cloned_repositories = git_handler.clone_repositories(CLONEDIR,
                                repositories_to_clone)

        printlog.info("Number of cloned repositories: %s", len(cloned_repositories))

        storage_handler.write(output[0], cloned_repositories)
        storage_handler.write(output[2], "")


rule download_skiplist_from_catalog:
    input:
        WORKDIR + "/.done_search_repositories",

    params:
        file_skiplist  = WORKDIR + "/skips.json",
        file_blacklist = WORKDIR + "/blacklist.txt",

    output:
        WORKDIR + "/skips.json",
        WORKDIR + "/blacklist.txt",
        WORKDIR + "/skiplist_all.txt",
        WORKDIR + "/.done_download_skiplist_from_catalog"
    run:
        util.download_http_file(SKIPS, output[0])
        util.download_http_file(BLACKLIST, output[1])

        merged_skiplist = util.merge_skiplist(file_blacklist=output[1],
                          file_skiplist=output[0])

        storage_handler.write(output[2], merged_skiplist)
        storage_handler.write(output[3], "")


rule search_repositories:
    input:
        WORKDIR + "/search_queries.txt"

    params:
        json_files = expand(WORKDIR + "/search_query_result_{start_date}_{end_date}.json", zip, start_date=START_DATES, end_date=END_DATES)

    output:
        WORKDIR + "/.done_search_repositories",
        WORKDIR + "/repository_list_from_search.txt",
        expand(WORKDIR + "/search_query_result_{start_date}_{end_date}.json", zip, start_date=START_DATES, end_date=END_DATES)

    run:
        json_data = None

        query_strings = storage_handler.read(input[0], multiple_lines=True)
        for query_string in query_strings:
            date_range  = query_string.split("created:")[1].split("..")
            start_date  = date_range[0]
            end_date    = date_range[1]
            output_file = f"{WORKDIR}/search_query_result_{start_date}_{end_date}.json"

            # If the output file exists, this is a case that the rule was
            # stopped in the middle of the execution. Existing output file
            # means that the search for the query was already done.
            # Accordingly, we skip this query.
            if os.path.exists(output_file):
                continue

            result = git_handler.search_repositories(query=query_string)
            storage_handler.write(output_file, result, to_json=True)

        items = []
        # create repository_list_from_search.txt
        for json_file in params.json_files:
            json_data = storage_handler.read(json_file, from_json=True)
            for item in json_data["items"]:
                items.append(item["full_name"])

        storage_handler.write(output[1], items)
        storage_handler.write(output[0], "")


rule generate_search_queries:
    input:
        WORKDIR + "/.initialized"
    output:
        WORKDIR + "/search_queries.txt"
    params:
        dates=search_date_ranges
    run:
        query_strings = []
        for date_range in params.dates:
            # snakemake workflow in:readme archived:false created:2024-11-01..2024-12-31
            query_string = f"{GIT_SEARCH_QUERY} created:{date_range[0]}..{date_range[1]}"
            query_strings.append(query_string)

        storage_handler.write(output[0], query_strings)

rule make_work_directory:
    """ Create a work directory as ./results/{GIT_PROVIDER}_{GIT_SEARCH_START_DATE}_{GIT_SEARCH_END_DATE} """
    output:
        WORKDIR + "/.initialized"
    run:
        os.makedirs(f"results/{GIT_PROVIDER}_{GIT_SEARCH_START_DATE}_{GIT_SEARCH_END_DATE}", exist_ok=True)
        # create .initialized file in the directory.
        storage_handler.write(output[0], "")
