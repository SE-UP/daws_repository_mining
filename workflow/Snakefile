import os
import scripts.util as util
import scripts.storage as storage
import scripts.database as database
import scripts.git_providers as git_providers
import scripts.git_analyzer as git_analyzer

configfile: "config.yaml"

printlog = util.setup_logger()


def _get_snakemake_pid(pid=None, i=None):
    import psutil
    if pid is None:
        pid = os.getpid()

    if i == None:
        i = 0

    current_process = psutil.Process(pid)
    process_name = current_process.name()

    if process_name == "snakemake":
        return current_process.pid
    else:
        parent_process = current_process.parent()
        if parent_process:
            if i > 3:
                printlog.error("Failed to get the snakemake PID.")
                raise ValueError("Failed to get the snakemake PID.")
            else:
                i += 1
            new_pid = _get_snakemake_pid(parent_process.pid, i)
            return new_pid


WORKSPACE_NAME     = config["workspace_name"]
ANALYSIS_FOR       = config["analysis_for"]
TARGET_LIST_TYPE   = config["target_list_type"]
TARGET_LIST_ARGS   = config["target_list_args"]
GIT_PROVIDER       = config["git_provider"]
GIT_PROVIDER_TOKEN = config["git_provider_token"]
RESULTDIR          = config["result_dir"]
WORKDIR            = f"{RESULTDIR}/{WORKSPACE_NAME}/"
CLONEDIR           = WORKDIR + "cloned_repositories/"
METADATADIR        = WORKDIR + "metadata/"
NOW                = util.now()
HTTP_TIMEOUT       = config["http_timeout"]
INTERVAL_APICALL   = config["interval_apicall"]
INTERVAL_CLONE     = config["interval_clone"]


DONE_CLONE_REPOSITORIES_FILE                = WORKDIR + config["metadata_files"]["done_clone_repositories"]
DONE_CALL_APIS_FILE                         = WORKDIR + config["metadata_files"]["done_call_apis"]
DONE_EXTRACT_COMMITS_FILE                   = WORKDIR + config["metadata_files"]["done_extract_commits"]
DONE_EXTEND_REPOSITORIES_INFO_FILE          = WORKDIR + config["metadata_files"]["done_extend_repositories_info"]
REPOSITORIES_TO_CLONE_FILE                  = WORKDIR + config["metadata_files"]["repositories_to_clone"]
CLONED_REPOSITORIES_FILE                    = WORKDIR + config["metadata_files"]["cloned_repositories"]
EXTRACTED_REPOSITORIES_FILE                 = WORKDIR + config["metadata_files"]["extracted_repositories"]
EXTENDED_REPOSITORIES_FILE                  = WORKDIR + config["metadata_files"]["extended_repositories"]
QUERIED_REPOSITORIES_FILE                   = WORKDIR + config["metadata_files"]["queried_repositories"]
FAILED_REPOSITORIES_TO_CLONE_FILE           = WORKDIR + config["metadata_files"]["failed_repositories_to_clone"]
FAILED_REPOSITORIES_TO_CALL_APIS_FILE       = WORKDIR + config["metadata_files"]["failed_repositories_to_call_apis"]
FAILED_REPOSITORIES_TO_EXTRACT_COMMITS_FILE = WORKDIR + config["metadata_files"]["failed_repositories_to_extract_commits"]
FAILED_REPOSITORIES_TO_EXTEND_INFO_FILE     = WORKDIR + config["metadata_files"]["failed_repositories_to_extend_info"]

storage_handler = storage.Storage(logger=printlog, engine="file", storage_config={"is_absolute": True, "mkdir_ok": True})

git_provider = git_providers.GitProvider(
    provider=GIT_PROVIDER,
    token=GIT_PROVIDER_TOKEN,
    checkout=config["checkout_existing_repositories"],
    logger=printlog,
    http_timeout=HTTP_TIMEOUT,
    interval_apicall=INTERVAL_APICALL,
    interval_clone=INTERVAL_CLONE)


# TODO: Implement the following rules
# rule migrate_jsonfiles_to_database:
# rule generate_event_logs_for_evolution_cycles:


rule extend_repositories_info:
    input:
        DONE_EXTRACT_COMMITS_FILE,
        CLONED_REPOSITORIES_FILE,

    output:
        DONE_EXTEND_REPOSITORIES_INFO_FILE,
        EXTENDED_REPOSITORIES_FILE,
        FAILED_REPOSITORIES_TO_EXTEND_INFO_FILE,

    run:
        cloned_repositories   = storage_handler.read(input[1], multiple_lines=True)
        failed_repositories   = []
        extended_repositories = []

        for i_repo, repo_fullname in enumerate(cloned_repositories):
            owner, repo_name = repo_fullname.strip().split("/")
            cloned_dir = os.path.join(WORKDIR, owner, repo_name)
            new_metadata_repo_info_file = cloned_dir + config["metadata_files"]["extended_repositories_info_suffix"]
            old_metadata_repo_info_file = cloned_dir + config["metadata_files"]["api_result_repo_info_suffix"]

            metadata_cicd_artifacts_file     = cloned_dir + config["metadata_files"]["api_result_cicd_artifacts_suffix"]
            metadata_cicd_workflow_runs_file = cloned_dir + config["metadata_files"]["api_result_cicd_workflow_runs_suffix"]
            metadata_cicd_workflows_file     = cloned_dir + config["metadata_files"]["api_result_cicd_workflows_suffix"]
            metadata_commit_comments_file    = cloned_dir + config["metadata_files"]["api_result_commit_comments_suffix"]
            metadata_commits_file            = cloned_dir + config["metadata_files"]["extracted_commits_suffix"]
            metadata_issue_comments_file     = cloned_dir + config["metadata_files"]["api_result_issue_comments_suffix"]
            metadata_issue_events_file       = cloned_dir + config["metadata_files"]["api_result_issue_events_suffix"]
            metadata_issues_file             = cloned_dir + config["metadata_files"]["api_result_issues_suffix"]
            metadata_pages_file              = cloned_dir + config["metadata_files"]["api_result_pages_suffix"]
            metadata_pullrequests_file       = cloned_dir + config["metadata_files"]["api_result_pullrequests_suffix"]
            metadata_releases_file           = cloned_dir + config["metadata_files"]["api_result_releases_suffix"]

            if not storage_handler.exists(metadata_commits_file):
                printlog.info("(%d/%d) No extracted commits for %s, skipping.",
                                i_repo+1, len(cloned_repositories), repo_fullname)
                failed_repositories.append(repo_fullname)
                continue

            if not storage_handler.exists(old_metadata_repo_info_file):
                printlog.info("(%d/%d) No Repositories API result for %s, skipping.",
                                i_repo+1, len(cloned_repositories), repo_fullname)
                failed_repositories.append(repo_fullname)
                continue

            if storage_handler.exists(new_metadata_repo_info_file):
                if config["force_extend_repositories_info"]:
                    printlog.info("(%d/%d) Already extended but re-run again for %s",
                                    i_repo+1, len(cloned_repositories), repo_fullname)
                    storage_handler.delete_file(new_metadata_repo_info_file)
                else:
                    printlog.info("(%d/%d) Extending repository info is already done for %s.",
                                    i_repo+1, len(cloned_repositories), repo_fullname)
                    extended_repositories.append(repo_fullname)
                    continue

            repo_info_from_api = storage_handler.read(old_metadata_repo_info_file, from_json=True)
            # Remove all keys ending with "_url"
            repo_info_from_api = {k: v for k, v in repo_info_from_api.items() if not k.endswith("_url")}

            printlog.info("(%d/%d) Processing %s...",
                            i_repo+1, len(cloned_repositories), repo_fullname)

            new_repo_info = repo_info_from_api
            new_repo_info["owner"] = owner

            commits            = storage_handler.read(metadata_commits_file, from_json=True)
            issues             = []
            issue_events       = []
            issue_comments     = []
            pull_requests      = []
            commit_comments    = []
            cicd_artifacts     = []
            cicd_workflows     = []
            cicd_workflow_runs = []
            releases           = []
            pages              = []

            if storage_handler.exists(metadata_issues_file):
                issues = storage_handler.read(metadata_issues_file, from_json=True)

            if storage_handler.exists(metadata_issue_events_file):
                issue_events = storage_handler.read(metadata_issue_events_file, from_json=True)

            if storage_handler.exists(metadata_issue_comments_file):
                issue_comments = storage_handler.read(metadata_issue_comments_file, from_json=True)

            if storage_handler.exists(metadata_pullrequests_file):
                pull_requests = storage_handler.read(metadata_pullrequests_file, from_json=True)

            if storage_handler.exists(metadata_commit_comments_file):
                commit_comments = storage_handler.read(metadata_commit_comments_file, from_json=True)

            if storage_handler.exists(metadata_cicd_artifacts_file):
                cicd_artifacts = storage_handler.read(metadata_cicd_artifacts_file, from_json=True)

            if storage_handler.exists(metadata_cicd_workflows_file):
                cicd_workflows = storage_handler.read(metadata_cicd_workflows_file, from_json=True)

            if storage_handler.exists(metadata_cicd_workflow_runs_file):
                cicd_workflow_runs = storage_handler.read(metadata_cicd_workflow_runs_file, from_json=True)

            if storage_handler.exists(metadata_releases_file):
                releases = storage_handler.read(metadata_releases_file, from_json=True)

            if storage_handler.exists(metadata_pages_file):
                pages = storage_handler.read(metadata_pages_file, from_json=True)

            new_repo_info["x_total_commits"]            = len(commits)
            new_repo_info["x_total_issues"]             = len(issues)
            new_repo_info["x_total_issue_events"]       = len(issue_events)
            new_repo_info["x_total_issue_comments"]     = len(issue_comments)
            new_repo_info["x_total_pull_requests"]      = len(pull_requests)
            new_repo_info["x_total_commit_comments"]    = len(commit_comments)
            new_repo_info["x_total_cicd_artifacts"]     = len(cicd_artifacts)
            new_repo_info["x_total_cicd_workflows"]     = len(cicd_workflows)
            new_repo_info["x_total_cicd_workflow_runs"] = len(cicd_workflow_runs)
            new_repo_info["x_total_releases"]           = len(releases)
            new_repo_info["x_total_pages"]              = len(pages)

            gitrepo_handler = git_analyzer.GitAnalysis(logger=printlog, repo_path=cloned_dir)

            new_repo_info = gitrepo_handler.extend_repo_info_general(
                repo_info=new_repo_info,
                commits=commits,
            )

            extra_method_name = f"extend_repo_info_for_{ANALYSIS_FOR}"
            try:
                extra_method = getattr(gitrepo_handler, extra_method_name)
            except AttributeError:
                raise ValueError(f"No method in GitAnalysis class for {ANALYSIS_FOR} analysis.")

            new_repo_info = extra_method(repo_info=new_repo_info, commits=commits)

            storage_handler.write(new_metadata_repo_info_file, new_repo_info, to_json=True)
            extended_repositories.append(repo_fullname)
            printlog.info("(%d/%d) Saved extended repository info for %s",
                            i_repo+1, len(cloned_repositories), repo_fullname)

        storage_handler.write(output[1], extended_repositories)
        storage_handler.write(output[2], failed_repositories)
        storage_handler.write(output[0], "")


rule extract_commits_from_repositories:
    input:
        DONE_CALL_APIS_FILE,
        CLONED_REPOSITORIES_FILE,

    output:
        DONE_EXTRACT_COMMITS_FILE,
        EXTRACTED_REPOSITORIES_FILE,
        FAILED_REPOSITORIES_TO_EXTRACT_COMMITS_FILE,

    run:
        cloned_repositories    = storage_handler.read(input[1], multiple_lines=True)
        extracted_repositories = []
        failed_repositories    = []

        for i, repo_fullname in enumerate(cloned_repositories):
            owner, repo_name = repo_fullname.strip().split("/")
            cloned_dir = os.path.join(WORKDIR, owner, repo_name)
            metadata_commits_file = cloned_dir + config["metadata_files"]["extracted_commits_suffix"]

            if storage_handler.exists(metadata_commits_file):
                if config["force_extract_commits"]:
                    printlog.info("(%d/%d) Already done but forcing to extract commits for %s",
                                  i + 1, len(cloned_repositories), repo_fullname)
                    storage_handler.delete_file(metadata_commits_file)

                else:
                    printlog.info("(%d/%d) Extracting commits is already done for %s.",
                                      i + 1, len(cloned_repositories), repo_fullname)
                    extracted_repositories.append(repo_fullname)
                    continue

            if not storage_handler.exists(cloned_dir):
                printlog.error("(%d/%d) Repository %s is not cloned.",
                                i + 1, len(cloned_repositories), repo_fullname)
                failed_repositories.append(repo_fullname)
                continue

            try:
                gitrepo_handler = git_analyzer.GitAnalysis(logger=printlog, repo_path=cloned_dir)
                count_commits = gitrepo_handler.extract_commits(analysis_for=ANALYSIS_FOR)
                printlog.info("(%d/%d) Extracted %d commits for %s",
                                i + 1, len(cloned_repositories), count_commits, repo_fullname)

                storage_handler.write(metadata_commits_file, gitrepo_handler.commits, to_json=True)
                extracted_repositories.append(repo_fullname)
            except Exception as e:
                printlog.error("(%d/%d) Failed to extract commits for %s: %s",
                                i + 1, len(cloned_repositories), repo_fullname, e)
                failed_repositories.append(repo_fullname)
                continue

        storage_handler.write(output[1], extracted_repositories)
        storage_handler.write(output[2], failed_repositories)
        storage_handler.write(output[0], "")


rule call_git_provider_apis:
    input:
        DONE_CLONE_REPOSITORIES_FILE,
        CLONED_REPOSITORIES_FILE,

    output:
        QUERIED_REPOSITORIES_FILE,
        FAILED_REPOSITORIES_TO_CALL_APIS_FILE,
        DONE_CALL_APIS_FILE

    run:
        cloned_repositories = storage_handler.read(input[1], multiple_lines=True)
        queried_repositories = []
        failed_repositories_to_call_apis = []

        for i, repo_fullname in enumerate(cloned_repositories):
            owner, repo = repo_fullname.strip().split("/")
            cloned_dir = os.path.join(WORKDIR, owner, repo)
            done_file = cloned_dir + config["metadata_files"]["done_call_apis_suffix"]
            metadata_repo_info_file = cloned_dir + config["metadata_files"]["api_result_repo_info_suffix"]
            metadata_issues_file = cloned_dir + config["metadata_files"]["api_result_issues_suffix"]
            metadata_pullrequests_file = cloned_dir + config["metadata_files"]["api_result_pullrequests_suffix"]
            metadata_issue_events_file = cloned_dir + config["metadata_files"]["api_result_issue_events_suffix"]
            metadata_issue_comments_file = cloned_dir + config["metadata_files"]["api_result_issue_comments_suffix"]
            metadata_commit_comments_file = cloned_dir + config["metadata_files"]["api_result_commit_comments_suffix"]
            metadata_cicd_artifacts_file = cloned_dir + config["metadata_files"]["api_result_cicd_artifacts_suffix"]
            metadata_cicd_workflows_file = cloned_dir + config["metadata_files"]["api_result_cicd_workflows_suffix"]
            metadata_cicd_workflow_runs_file = cloned_dir + config["metadata_files"]["api_result_cicd_workflow_runs_suffix"]
            metadata_releases_file = cloned_dir + config["metadata_files"]["api_result_releases_suffix"]
            metadata_pages_file = cloned_dir + config["metadata_files"]["api_result_pages_suffix"]

            if storage_handler.exists(done_file):
                if config["force_call_apis"]:
                    printlog.info("(%d/%d) Already done but forcing to call API for %s",
                                   i + 1, len(cloned_repositories), repo_fullname)

                else:
                    printlog.info("(%d/%d) Calling APIs is already done for %s.",
                                   i + 1, len(cloned_repositories), repo_fullname)
                    queried_repositories.append(repo_fullname)
                    continue

            if not storage_handler.exists(cloned_dir):
                printlog.error("(%d/%d) Repository %s is not cloned.",
                                i + 1, len(cloned_repositories), repo_fullname)
                failed_repositories_to_call_apis.append(repo_fullname)
                continue

            try:
                # Call Repositories API
                printlog.info("(%d/%d) Calling repository API for %s",
                                i + 1, len(cloned_repositories), repo_fullname)
                repo_info = (git_provider.call_api_repository(owner, repo))[0]
                storage_handler.write(metadata_repo_info_file, repo_info, to_json=True)
                printlog.info("(%d/%d) Saved repo_info for %s",
                                i + 1, len(cloned_repositories), repo_fullname)

                # Call Issues API
                issues = []
                pull_requests = []
                issue_comments = []
                commit_comments = []
                issue_events = []
                cicd_artifacts = []
                cicd_workflow_runs = []

                if repo_info.get("has_issues", False):
                    issues = git_provider.call_api_issues(owner, repo)
                    if len(issues) > 0:
                        storage_handler.write(metadata_issues_file, issues, to_json=True)
                        printlog.info("(%d/%d) Saved %d issues for %s",
                                        i + 1, len(cloned_repositories), len(issues), repo_fullname)
                    else:
                        printlog.info("(%d/%d) No issues for %s",
                                        i + 1, len(cloned_repositories), repo_fullname)
                else:
                    printlog.info("(%d/%d) Issues are not enabled for %s",
                                    i + 1, len(cloned_repositories), repo_fullname)

                if len(issues) > 0:
                    # Call Pull Requests API
                    issues_pr = [issue for issue in issues if issue.get("pull_request", None)]
                    if len(issues_pr) > 0:
                        for i_pr, issue in enumerate(issues_pr):
                            issue_number = issue["number"]
                            pull_requests.append(git_provider.call_api_pull_request(owner, repo, issue_number))
                            printlog.info("  * (%d/%d) Saved pull request info for issue no. %d.",
                                            i_pr + 1, len(issues_pr), issue_number)

                        storage_handler.write(metadata_pullrequests_file, pull_requests, to_json=True)

                    # Call Issue Comments API
                    issues_with_comments = [issue for issue in issues if issue.get("comments", None)]
                    if len(issues_with_comments) > 0:
                        for i_comment, issue in enumerate(issues_with_comments):
                            issue_number = issue["number"]
                            issue_comments.extend(git_provider.call_api_issue_comments(owner, repo, issue_number))
                            printlog.info("  * (%d/%d) Saved comments for issue no. %d.",
                                            i_comment + 1, len(issues_with_comments), issue_number)

                        storage_handler.write(metadata_issue_comments_file, issue_comments, to_json=True)

                    # Call Issue Events API
                    if len(issues) > 0:
                        for i_event, issue in enumerate(issues):
                            issue_number = issue["number"]
                            issue_events.extend(git_provider.call_api_issue_events(owner, repo, issue_number))
                            printlog.info("  * (%d/%d) Saved events for issue no. %d.",
                                            i_event + 1, len(issues), issue_number)

                        storage_handler.write(metadata_issue_events_file, issue_events, to_json=True)

                # Call Commit Comments API
                commit_comments = git_provider.call_api_commit_comments(owner, repo)
                if commit_comments:
                    storage_handler.write(metadata_commit_comments_file, commit_comments, to_json=True)
                    printlog.info("(%d/%d) Saved %d commit comments for %s",
                                    i + 1, len(cloned_repositories), len(commit_comments), repo_fullname)
                else:
                    printlog.info("(%d/%d) No commit comments for %s",
                                    i + 1, len(cloned_repositories), repo_fullname)


                # Call CI/CD Artifacts API
                cicd_artifacts = git_provider.call_api_cicd_artifacts(owner, repo)
                if cicd_artifacts:
                    storage_handler.write(metadata_cicd_artifacts_file, cicd_artifacts, to_json=True)
                    printlog.info("(%d/%d) Saved %d CI/CD artifacts for %s",
                                    i + 1, len(cloned_repositories), len(cicd_artifacts), repo_fullname)
                else:
                    printlog.info("(%d/%d) No CI/CD artifacts for %s",
                                    i + 1, len(cloned_repositories), repo_fullname)

                # Call CI/CD Workflows API
                cicd_workflows = git_provider.call_api_cicd_workflows(owner, repo)
                if cicd_workflows:
                    storage_handler.write(metadata_cicd_workflows_file, cicd_workflows, to_json=True)
                    printlog.info("(%d/%d) Saved %d CI/CD workflows for %s",
                                    i + 1, len(cloned_repositories), len(cicd_workflows), repo_fullname)
                else:
                    printlog.info("(%d/%d) No CI/CD workflows for %s",
                                    i + 1, len(cloned_repositories), repo_fullname)

                # Call CI/CD Workflow Runs API
                cicd_workflow_runs = git_provider.call_api_cicd_workflow_runs(owner, repo)
                if cicd_workflow_runs:
                    storage_handler.write(metadata_cicd_workflow_runs_file, cicd_workflow_runs, to_json=True)
                    printlog.info("(%d/%d) Saved %d CI/CD workflow runs for %s",
                                    i + 1, len(cloned_repositories), len(cicd_workflow_runs), repo_fullname)
                else:
                    printlog.info("(%d/%d) No CI/CD workflow runs for %s",
                                    i + 1, len(cloned_repositories), repo_fullname)

                # Call Releases API
                releases = git_provider.call_api_releases(owner, repo)
                if releases:
                    metadata_releases_file = cloned_dir + config["metadata_files"]["api_result_releases_suffix"]
                    storage_handler.write(metadata_releases_file, releases, to_json=True)
                    printlog.info("(%d/%d) Saved %d releases for %s",
                                    i + 1, len(cloned_repositories), len(releases), repo_fullname)
                else:
                    printlog.info("(%d/%d) No releases for %s",
                                    i + 1, len(cloned_repositories), repo_fullname)

                # Call Pages API
                if repo_info.get("has_pages", False):
                    pages = git_provider.call_api_pages(owner, repo)
                    if pages:
                        metadata_pages_file = cloned_dir + config["metadata_files"]["api_result_pages_suffix"]
                        storage_handler.write(metadata_pages_file, pages, to_json=True)
                        printlog.info("(%d/%d) Saved %d pages for %s",
                                        i + 1, len(cloned_repositories), len(pages), repo_fullname)
                    else:
                        printlog.info("(%d/%d) No pages for %s",
                                        i + 1, len(cloned_repositories), repo_fullname)
                else:
                    printlog.info("(%d/%d) Pages are not enabled for %s",
                                    i + 1, len(cloned_repositories), repo_fullname)

                queried_repositories.append(repo_fullname)
                storage_handler.write(done_file, "")

            except Exception as e:
                printlog.error("(%d/%d) Failed to call API for %s: %s",
                                i + 1, len(cloned_repositories), repo_fullname, e)
                failed_repositories_to_call_apis.append(repo_fullname)
                continue

        printlog.info("A total of queried repositories: %d", len(queried_repositories))
        printlog.info("A total of failed repositories: %d", len(failed_repositories_to_call_apis))
        storage_handler.write(output[0], queried_repositories)
        storage_handler.write(output[1], failed_repositories_to_call_apis)
        storage_handler.write(output[2], "")


rule clone_repositories:
    input:
        REPOSITORIES_TO_CLONE_FILE,

    output:
        CLONED_REPOSITORIES_FILE,
        FAILED_REPOSITORIES_TO_CLONE_FILE,
        DONE_CLONE_REPOSITORIES_FILE,

    run:
        repositories_to_clone = storage_handler.read(input[0], multiple_lines=True)
        failed_repositories_to_clone = []
        cloned_repositories = []
        printlog.info("A number of repositories to clone: %d", len(repositories_to_clone))

        for i, repo_fullname in enumerate(repositories_to_clone):
            owner, repo_name = repo_fullname.strip().split("/")
            dir_to_clone = os.path.join(WORKDIR, owner, repo_name)
            done_file = dir_to_clone + config["metadata_files"]["done_clone_repository_suffix"]

            if storage_handler.exists(done_file):
                if config["checkout_existing_repositories"]:
                    storage_handler.delete_file(done_file)
                    printlog.info("(%d/%d) Checking out %s/%s to %s", i + 1, len(repositories_to_clone), owner, repo_name, dir_to_clone)
                    try:
                        git_provider.clone_repository(dir_to_clone, owner, repo_name)
                        storage_handler.write(done_file, "")
                        cloned_repositories.append(repo_fullname)
                    except Exception as e:
                        printlog.error("Failed to clone %s/%s: %s", owner, repo_name, e)
                        failed_repositories_to_clone.append(repo_fullname)
                        continue
                else:
                    printlog.info("Repository %s/%s is already cloned.", owner, repo_name)
                    cloned_repositories.append(repo_fullname)
                    continue
            else:
                printlog.info("(%d/%d) Cloning %s/%s to %s", i + 1, len(repositories_to_clone), owner, repo_name, dir_to_clone)
                try:
                    git_provider.clone_repository(dir_to_clone, owner, repo_name)
                    storage_handler.write(done_file, "")
                    cloned_repositories.append(repo_fullname)
                except Exception as e:
                    printlog.error("Failed to clone %s/%s: %s (No .clone.done file!)", owner, repo_name, e)
                    printlog.error("Manually delete the directory %s or create the %s file to mark it as cloned.", dir_to_clone, done_file)
                    failed_repositories_to_clone.append(repo_fullname)
                    continue

        printlog.info("A total of cloned repositories: %d", len(cloned_repositories))
        printlog.info("A total of failed repositories: %d", len(failed_repositories_to_clone))

        if len(cloned_repositories) == 0:
            printlog.error("No repositories were cloned successfully. Please check the logs.")
            raise ValueError("No repositories were cloned successfully.")

        storage_handler.write(output[0], cloned_repositories)
        storage_handler.write(output[1], failed_repositories_to_clone)
        storage_handler.write(output[2], "")


rule initialize_to_clone_repositories:
    output:
        REPOSITORIES_TO_CLONE_FILE,

    run:
        repositories_to_clone = []
        match TARGET_LIST_TYPE:
            case "file":
                for TARGET_LIST_ARG in TARGET_LIST_ARGS:
                    with open(TARGET_LIST_ARG, "r") as f:
                        for line in f:
                            repositories_to_clone.append(line.strip())

            case "name":
                repositories_to_clone = TARGET_LIST_ARGS

            case "search_query":
                search_result = git_provider.search_repositories(query=query_string)
                if 'items' in search_result:
                    for item in search_result["items"]:
                        repositories_to_clone.append(item["full_name"])
                else:
                    printlog.error("No items found in search result: query=%s: result=%s", query_string, search_result)
                    raise ValueError("No items found in search result.")

            case _:
                printlog.error("Invalid TARGET_LIST_TYPE: %s: Should be 'file', 'search_query', or 'name'", TARGET_LIST_TYPE)
                raise ValueError("Invalid TARGET_LIST_TYPE")

        printlog.info("A number of repositories to clone: %d", len(repositories_to_clone))

        storage_handler.write(output[0], repositories_to_clone)


rule cleanup:
    run:
        files_to_delete = [
            CLONED_REPOSITORIES_FILE,
            REPOSITORIES_TO_CLONE_FILE,
            DONE_CLONE_REPOSITORIES_FILE,
            QUERIED_REPOSITORIES_FILE,
            DONE_CALL_APIS_FILE,
            FAILED_REPOSITORIES_TO_CALL_APIS_FILE,
            FAILED_REPOSITORIES_TO_CLONE_FILE,
            DONE_EXTRACT_COMMITS_FILE,
            EXTRACTED_REPOSITORIES_FILE,
            FAILED_REPOSITORIES_TO_EXTRACT_COMMITS_FILE,
            DONE_EXTEND_REPOSITORIES_INFO_FILE,
            EXTENDED_REPOSITORIES_FILE,
            FAILED_REPOSITORIES_TO_EXTEND_INFO_FILE,
        ]

        printlog.info("Cleaning up the session files.")
        for file in files_to_delete:
            if storage_handler.exists(file):
                try:
                    printlog.info("Deleting %s", file)
                    storage_handler.delete_file(file)
                except Exception as e:
                    printlog.error("Failed to delete %s: %s", file, e)
            else:
                printlog.info("Not found %s, skipping.", file)
