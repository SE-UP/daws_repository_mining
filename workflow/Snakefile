import json
import scripts.util as util
import scripts.database as database
import scripts.git_providers as git_providers

configfile: "config.yaml"

printlog = util.setup_logger()

GIT_SEARCH_START_DATE_STR = config["search_start_date"]
GIT_SEARCH_START_DATE     = util.str_to_date(GIT_SEARCH_START_DATE_STR)
GIT_SEARCH_END_DATE_STR   = config["search_end_date"]
GIT_SEARCH_END_DATE       = util.str_to_date(GIT_SEARCH_END_DATE_STR)
GIT_SEARCH_INTERVAL       = config["search_interval"]
GIT_SEARCH_QUERY          = config["search_query"]
GIT_PROVIDER              = config["git_provider"]
GIT_PROVIDER_TOKEN        = config["git_provider_token"]
DB_ENGINE                 = config["database"]["engine"]
DB_CONFIG                 = config["database_config"][DB_ENGINE]
BLACKLIST                 = config["snakemake-catalog-urls"]["blacklist"]
SKIPS                     = config["snakemake-catalog-urls"]["skips"]

WORKDIR = config["result_dir"] + f"/{GIT_PROVIDER}_{GIT_SEARCH_START_DATE_STR}_{GIT_SEARCH_END_DATE_STR}"
CLONEDIR = config["result_dir"] + f"/{GIT_PROVIDER}_{GIT_SEARCH_START_DATE_STR}_{GIT_SEARCH_END_DATE_STR}/cloned_repositories"
NOW     = util.now()

search_date_ranges = util.generate_date_ranges(GIT_SEARCH_START_DATE, GIT_SEARCH_END_DATE, GIT_SEARCH_INTERVAL)
START_DATES = [date_range[0] for date_range in search_date_ranges]
END_DATES   = [date_range[1] for date_range in search_date_ranges]
printlog.debug(f"search start dates: {START_DATES}")
printlog.debug(f"search end dates: {END_DATES}")


rule migrate_jsonfiles_to_database:
    input:
        WORKDIR + "/.done_clone_repositories",
        WORKDIR + "/cloned_repositories.txt",
        WORKDIR + "/repositories_to_clone.txt",

    params:
        json_files = expand(WORKDIR + "/search_query_result_{start_date}_{end_date}.json", zip, start_date=START_DATES, end_date=END_DATES),
        skiplist_file = WORKDIR + "/skiplist_all.txt"

    output:
        WORKDIR + "/.done_migrate_jsonfiles_to_database",
        WORKDIR + "/migrated_repositories.txt"

    run:
        # Get the list of cloned repositories
        cloned_repositories = []
        repositories_to_clone = []
        with open(input[1], 'r', encoding='utf-8') as f:
            for line in f:
                cloned_repositories.append(line.strip())

        with open(input[2], 'r', encoding='utf-8') as f:
            for line in f:
                repositories_to_clone.append(line.strip())

        for repo in repositories_to_clone:
            if repo not in cloned_repositories:
                printlog.warning("Repository not cloned: " + repo)

        db = database.Database(logger=printlog, engine=DB_ENGINE,
                                git_provider=GIT_PROVIDER, db_config=DB_CONFIG)

        stored_repositories = []
        for json_file in params.json_files:
            with open(f"{json_file}", 'r', encoding='utf-8') as f:
                api_response = json.load(f)
                if "items" in api_response and len(api_response["items"]) > 0:
                    stored_repositories = db.store_search_repositories_results(
                                            api_response, cloned_repositories)
                else:
                    printlog.debug(f"No items in {json_file}")

        db.close()

        # create migrated_repositories.txt
        with open(output[1], 'w', encoding='utf-8') as f:
            for full_name in stored_repositories:
                f.write(f"{full_name}\n")

        # create .done_migrate_jsonfiles_to_database file
        with open(output[0], 'w', encoding='utf-8') as f:
            f.write("")


rule clone_repositories:
    input:
        WORKDIR + "/.done_download_skiplist_from_catalog",
        WORKDIR + "/skiplist_all.txt"

    params:
        repolist_file = WORKDIR + "/repository_list_from_search.txt"

    output:
        WORKDIR + "/cloned_repositories.txt",
        WORKDIR + "/repositories_to_clone.txt",
        WORKDIR + "/.done_clone_repositories"

    run:
        skiplist_all = []
        repositories_to_clone = []

        with open(input[1], 'r', encoding='utf-8') as f:
            for line in f:
                skiplist_all.append(line.strip())

        skip_count = 0
        target_count = 0
        with open(params.repolist_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip() not in skiplist_all:
                    repositories_to_clone.append(line.strip())
                    target_count += 1
                else:
                    skip_count += 1

        printlog.info("Number of repositories to clone: %s (skipped: %s)" %
                                                (target_count, skip_count))

        with open(output[1], 'w', encoding='utf-8') as f:
            for full_name in repositories_to_clone:
                f.write(f"{full_name}\n")

        git_provider = git_providers.GitProvider(
                provider=GIT_PROVIDER,
                token=GIT_PROVIDER_TOKEN,
                logger=printlog)

        cloned_repositories = git_provider.clone_repositories(CLONEDIR,
                                repositories_to_clone)

        printlog.info("Number of cloned repositories: %s" % len(cloned_repositories))

        # create cloned_repositories.txt
        with open(output[0], 'w', encoding='utf-8') as f:
            for full_name in cloned_repositories:
                f.write(f"{full_name}\n")

        # create .done_clone_repositories file
        with open(output[2], 'w', encoding='utf-8') as f:
            f.write("")


rule download_skiplist_from_catalog:
    input:
        WORKDIR + "/.done_search_repositories",

    params:
        file_skiplist  = WORKDIR + "/skips.json",
        file_blacklist = WORKDIR + "/blacklist.txt",

    output:
        WORKDIR + "/skips.json",
        WORKDIR + "/blacklist.txt",
        WORKDIR + "/skiplist_all.txt",
        WORKDIR + "/.done_download_skiplist_from_catalog"
    run:
        util.download_http_file(SKIPS, output[0])
        util.download_http_file(BLACKLIST, output[1])

        merged_skiplist = util.merge_skiplist(file_blacklist=output[1],
                          file_skiplist=output[0])

        # create skiplist_all.txt
        with open(output[2], 'w', encoding='utf-8') as f:
            for item in merged_skiplist:
                f.write(f"{item}\n")

        # create .done_download_skiplist_from_catalog file
        with open(output[3], 'w', encoding='utf-8') as f:
            f.write("")


rule search_repositories:
    input:
        WORKDIR + "/search_queries.txt"

    params:
        json_files = expand(WORKDIR + "/search_query_result_{start_date}_{end_date}.json", zip, start_date=START_DATES, end_date=END_DATES)

    output:
        WORKDIR + "/.done_search_repositories",
        WORKDIR + "/repository_list_from_search.txt",
        expand(WORKDIR + "/search_query_result_{start_date}_{end_date}.json", zip, start_date=START_DATES, end_date=END_DATES)

    run:
        json_data = None
        git_provider = git_providers.GitProvider(
                provider=GIT_PROVIDER,
                token=GIT_PROVIDER_TOKEN,
                logger=printlog)

        with open(input[0], 'r', encoding='utf-8') as f:
            for line in f:
                query_string = line.strip()
                date_range  = query_string.split("created:")[1].split("..")
                start_date  = date_range[0]
                end_date    = date_range[1]
                output_file = f"{WORKDIR}/search_query_result_{start_date}_{end_date}.json"

                # If the output file exists, this is a case that the rule was
                # stopped in the middle of the execution. Existing output file
                # means that the search for the query was already done.
                # Accordingly, we skip this query.
                if os.path.exists(output_file):
                    continue

                result = git_provider.search_repositories(query=query_string)

                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(result, f, indent=4)

        # create repository_list_from_search.txt
        for json_file in params.json_files:
            with open(json_file, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
                with open(output[1], 'a', encoding='utf-8') as f:
                    for item in json_data["items"]:
                        f.write(f"{item['full_name']}\n")

        # create .done_search_repositories file
        with open(output[0], 'w', encoding='utf-8') as f:
            f.write("")


rule generate_search_queries:
    input:
        WORKDIR + "/.initialized"
    output:
        WORKDIR + "/search_queries.txt"
    params:
        dates=search_date_ranges
    run:
        with open(output[0], 'w', encoding='utf-8') as f:
            for date_range in params.dates:
                # snakemake workflow in:readme archived:false created:2024-11-01..2024-12-31
                query_string = f"{GIT_SEARCH_QUERY} created:{date_range[0]}..{date_range[1]}"
                f.write(f"{query_string}\n")


rule make_work_directory:
    """ Create a work directory as ./results/{GIT_PROVIDER}_{GIT_SEARCH_START_DATE}_{GIT_SEARCH_END_DATE} """
    output:
        WORKDIR + "/.initialized"
    run:
        os.makedirs(f"results/{GIT_PROVIDER}_{GIT_SEARCH_START_DATE}_{GIT_SEARCH_END_DATE}", exist_ok=True)
        # create .initialized file in the directory.
        with open(output[0], 'w', encoding='utf-8') as f:
            f.write("")
