import json
import scripts.util as util
import scripts.database as database
import scripts.git_providers as git_providers

configfile: "config.yaml"

printlog = util.setup_logger()

GIT_SEARCH_START_DATE_STR = config["search_start_date"]
GIT_SEARCH_START_DATE     = util.str_to_date(GIT_SEARCH_START_DATE_STR)
GIT_SEARCH_END_DATE_STR   = config["search_end_date"]
GIT_SEARCH_END_DATE       = util.str_to_date(GIT_SEARCH_END_DATE_STR)
GIT_SEARCH_INTERVAL       = config["search_interval"]
GIT_SEARCH_QUERY          = config["search_query"]
GIT_PROVIDER              = config["git_provider"]
GIT_PROVIDER_TOKEN        = config["git_provider_token"]
DB_ENGINE                 = config["database"]["engine"]
DB_CONFIG                 = config["database_config"][DB_ENGINE]

WORKDIR = config["result_dir"] + f"/{GIT_PROVIDER}_{GIT_SEARCH_START_DATE_STR}_{GIT_SEARCH_END_DATE_STR}"
NOW     = util.now()

search_date_ranges = util.generate_date_ranges(GIT_SEARCH_START_DATE, GIT_SEARCH_END_DATE, GIT_SEARCH_INTERVAL)
START_DATES = [date_range[0] for date_range in search_date_ranges]
END_DATES   = [date_range[1] for date_range in search_date_ranges]
printlog.debug(f"search start dates: {START_DATES}")
printlog.debug(f"search end dates: {END_DATES}")


rule migrate_jsonfiles_to_database:
    input:
        expand(WORKDIR + "/search_query_result_{start_date}_{end_date}.json", zip, start_date=START_DATES, end_date=END_DATES)
    output:
        WORKDIR + "/.done_migrate_jsonfiles_to_database"
    run:
        db = database.Database(logger=printlog, engine=DB_ENGINE, git_provider=GIT_PROVIDER,
                                db_config=DB_CONFIG)

        for input_file in input:
            with open(input_file, 'r') as f:
                json_data = json.load(f)
                db.store_search_repositories_results(json_data)
        db.close()

        # create .done_migrate_jsonfiles_to_database file
        with open(output[0], 'w') as f:
            f.write("")


rule search_repositories:
    input:
        WORKDIR + "/search_queries.txt"
    output:
        WORKDIR + "/.done_search_repositories"
    run:
        json_data = None
        git_provider = git_providers.GitProvider(
                provider=GIT_PROVIDER,
                token=GIT_PROVIDER_TOKEN,
                logger=printlog)

        with open(input[0], 'r') as f:
            for line in f:
                query_string = line.strip()
                date_range  = query_string.split("created:")[1].split("..")
                start_date  = date_range[0]
                end_date    = date_range[1]
                output_file = f"{WORKDIR}/search_query_result_{start_date}_{end_date}.json"

                # If the output file exists, this is a case that the rule was
                # stopped in the middle of the execution. Existing output file
                # means that the search for the query was already done.
                # Accordingly, we skip this query.
                if os.path.exists(output_file):
                    continue

                result = git_provider.search_repositories(query=query_string)

                with open(output_file, 'w') as f:
                    json.dump(result, f, indent=4)

        # create .done_search_repositories file
        with open(output[0], 'w') as f:
            f.write("")


rule generate_search_queries:
    input:
        WORKDIR + "/.initialized"
    output:
        WORKDIR + "/search_queries.txt"
    params:
        dates=search_date_ranges
    run:
        with open(output[0], 'w') as f:
            for date_range in params.dates:
                # snakemake workflow in:readme archived:false created:2024-11-01..2024-12-31
                query_string = f"{GIT_SEARCH_QUERY} created:{date_range[0]}..{date_range[1]}"
                f.write(f"{query_string}\n")


rule make_work_directory:
    """ make a work directory as ./results/{START_DATE}_{END_DATE} """
    output:
        WORKDIR + "/.initialized"
    run:
        os.makedirs(f"results/{GIT_SEARCH_START_DATE}_{GIT_SEARCH_END_DATE}", exist_ok=True)
        # create .initialized file in the directory.
        with open(output[0], 'w') as f:
            f.write("")
