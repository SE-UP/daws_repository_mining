import os
import scripts.util as util
import scripts.storage as storage
import scripts.database as database
import scripts.git_providers as git_providers
import scripts.analysis_git_repository as analysis_git

configfile: "config.yaml"

printlog = util.setup_logger()

GIT_SEARCH_START_DATE_STR = config["search_start_date"]
GIT_SEARCH_START_DATE     = util.str_to_datetime(GIT_SEARCH_START_DATE_STR).date()
GIT_SEARCH_END_DATE_STR   = config["search_end_date"]
GIT_SEARCH_END_DATE       = util.str_to_datetime(GIT_SEARCH_END_DATE_STR).date()
GIT_SEARCH_INTERVAL       = config["search_interval"]
GIT_SEARCH_QUERY          = config["search_query"]
GIT_PROVIDER              = config["git_provider"]
GIT_PROVIDER_TOKEN        = config["git_provider_token"]
DB_ENGINE                 = config["database"]["engine"]
DB_CONFIG                 = config["database_config"][DB_ENGINE]
BLACKLIST                 = config["snakemake-catalog-urls"]["blacklist"]
SKIPS                     = config["snakemake-catalog-urls"]["skips"]

WORKDIR     = config["result_dir"] + f"/{GIT_PROVIDER}_{GIT_SEARCH_START_DATE_STR}_{GIT_SEARCH_END_DATE_STR}"
CLONEDIR    = config["result_dir"] + f"/{GIT_PROVIDER}_{GIT_SEARCH_START_DATE_STR}_{GIT_SEARCH_END_DATE_STR}/cloned_repositories"
METADATADIR = config["result_dir"] + f"/{GIT_PROVIDER}_{GIT_SEARCH_START_DATE_STR}_{GIT_SEARCH_END_DATE_STR}/metadata"
NOW     = util.now()

storage_handler = storage.Storage(logger=printlog, engine="file", storage_config={"is_absolute": True, "mkdir_ok": True})

gitapi_handler = git_providers.GitProvider(
    provider=GIT_PROVIDER,
    token=GIT_PROVIDER_TOKEN,
    logger=printlog)

search_date_ranges = util.generate_date_ranges(GIT_SEARCH_START_DATE, GIT_SEARCH_END_DATE, GIT_SEARCH_INTERVAL)
START_DATES = [date_range[0] for date_range in search_date_ranges]
END_DATES   = [date_range[1] for date_range in search_date_ranges]
printlog.debug(f"search start dates: {START_DATES}")
printlog.debug(f"search end dates: {END_DATES}")


rule migrate_jsonfiles_to_database:
    input:
        WORKDIR + "/.done_clone_repositories",
        WORKDIR + "/cloned_repositories.txt",
        WORKDIR + "/repositories_to_clone.txt",
        WORKDIR + "/.done_get_issues",
        WORKDIR + "/.done_get_issues_extras",
        WORKDIR + "/.done_extract_git_repository_metadata",

    params:
        json_files = expand(WORKDIR + "/search_query_result_{start_date}_{end_date}.json", zip, start_date=START_DATES, end_date=END_DATES),
        skiplist_file = WORKDIR + "/skiplist_all.txt"

    output:
        WORKDIR + "/.done_migrate_jsonfiles_to_database",
        WORKDIR + "/migrated_repositories.txt"

    run:
        cloned_repositories = storage_handler.read(input[1], multiple_lines=True)
        repositories_to_clone = storage_handler.read(input[2], multiple_lines=True)

        for repo in repositories_to_clone:
            if repo not in cloned_repositories:
                printlog.warning("Repository not cloned: %s", repo)

        db = database.Database(logger=printlog, engine=DB_ENGINE,
                                git_provider=GIT_PROVIDER, db_config=DB_CONFIG)

        stored_repositories = []
        for json_file in params.json_files:
            api_response = storage_handler.read(json_file, from_json=True)
            if "items" in api_response and len(api_response["items"]) > 0:
                stored_repositories = db.store_search_repositories_results(
                                        api_response, cloned_repositories)
            else:
                printlog.debug("No items in %s", json_file)

        db.close()

        storage_handler.write(output[1], stored_repositories)
        storage_handler.write(output[0], "")


rule rebuild_git_repositories_info:
    input:
        WORKDIR + "/.done_extract_git_repository_metadata",
        WORKDIR + "/repositories_to_clone.txt",

    params:
        search_query_files = expand(WORKDIR + "/search_query_result_{start_date}_{end_date}.json", zip, start_date=START_DATES, end_date=END_DATES),

    output:
        WORKDIR + "/.done_rebuild_git_repositories_info",

    run:
        cloned_repositories = storage_handler.read(input[1], multiple_lines=True)

        repositories_info_from_git_provider = []
        for search_query_file in params.search_query_files:
            queries = storage_handler.read(search_query_file, from_json=True)
            repositories_info_from_git_provider += queries["items"]

        repositories_info_from_git_provider = util.convert_list_to_dict(
            src_list=repositories_info_from_git_provider,
            key="full_name"
        )

        for repo_fullname in cloned_repositories:
            owner, repo_name = repo_fullname.strip().split("/")
            file_commit_metadata = f"{METADATADIR}/{owner}_{repo_name}_commits.json"

            if not os.path.exists(file_commit_metadata):
                printlog.info("Metadata for commits is unavailable: %s.", repo_fullname)
                continue

            repo_info = {
                "repo_owner": owner,
                "repo_name": repo_name,
                "repo_full_name": repo_fullname,
            }

            new_repo_info = repo_info | repositories_info_from_git_provider[repo_fullname]

            commits = storage_handler.read(file_commit_metadata, from_json=True)

            new_repo_info["n_commits"] = len(commits)
            new_repo_info["n_commits_snakemake_related"] = 0
            new_repo_info["n_evalution_cycles"] = 0
            new_repo_info["evalution_cycles"] = []
            new_repo_info["first_commit_at_epoch"] = util.now(is_epoch=True)
            new_repo_info["last_commit_at_epoch"] = 0
            authors = set()
            committers = set()
            cycle_begin = {}
            cycle_end = {}
            cycle_n_commits = 0
            cycle_n_commits_snakemake_related = 0
            cycle_n_snakemake_rules_added = 0
            cycle_n_snakemake_rules_removed = 0
            cycle_n_snakemake_modules_added = 0
            cycle_n_snakemake_modules_removed = 0
            file_extensions = []
            for commit_hash in sorted(commits, key=lambda k: commits[k]['committer_epoch']):
                commit = commits[commit_hash]
                if "author" in commit:
                    authors.add(commit["author"])

                if "committer" in commit:
                    committers.add(commit["committer"])

                cycle_n_commits += 1
                if "snakemake_related" in commit and commit["snakemake_related"]:
                    new_repo_info["n_commits_snakemake_related"] += 1

                    cycle_current = {
                        "hash": commit_hash,
                        "epoch": commit["committer_epoch"],
                    }

                    if cycle_begin == {}:
                        cycle_begin = cycle_current
                        cycle_n_commits = 0
                    else:
                        cycle_end = cycle_current
                        cycle_n_commits_snakemake_related += 1
                        added_or_removed = (
                            commit["snakemake_n_rules_added"] -
                            commit["snakemake_n_rules_removed"] or
                            commit["snakemake_n_modules_added"] -
                            commit["snakemake_n_modules_removed"]
                        )
                        if added_or_removed:
                            new_repo_info["n_evalution_cycles"] += 1
                            if commit["snakemake_n_rules_added"] > 0:
                                cycle_n_snakemake_rules_added = commit["snakemake_n_rules_added"]
                            if commit["snakemake_n_rules_removed"] > 0:
                                cycle_n_snakemake_rules_removed = commit["snakemake_n_rules_removed"]
                            if commit["snakemake_n_modules_added"] > 0:
                                cycle_n_snakemake_modules_added = commit["snakemake_n_modules_added"]
                            if commit["snakemake_n_modules_removed"] > 0:
                                cycle_n_snakemake_modules_removed = commit["snakemake_n_modules_removed"]
                            if "file_extensions" in commit:
                                file_extensions += commit["file_extensions"]
                            diff_days = None
                            diff_mins = None
                            diff_hours = None
                            begin_epoch = cycle_begin["epoch"]
                            end_epoch   = cycle_end["epoch"]
                            if begin_epoch and end_epoch:
                                diff_days = util.convert_seconds(end_epoch - begin_epoch, unit="d")
                                diff_mins = util.convert_seconds(end_epoch - begin_epoch, unit="m")
                                diff_hours = util.convert_seconds(end_epoch - begin_epoch, unit="h")

                            new_repo_info["evalution_cycles"].append({
                                "begin": cycle_begin,
                                "end": cycle_end,
                                "diff_days": diff_days,
                                "diff_mins": diff_mins,
                                "diff_hours": diff_hours,
                                "n_commits": cycle_n_commits,
                                "n_commits_snakemake_related": cycle_n_commits_snakemake_related,
                                "n_snakemake_rules_added": cycle_n_snakemake_rules_added,
                                "n_snakemake_rules_removed": cycle_n_snakemake_rules_removed,
                                "n_snakemake_modules_added": cycle_n_snakemake_modules_added,
                                "n_snakemake_modules_removed": cycle_n_snakemake_modules_removed,
                                "file_extensions": list(set(file_extensions)),
                            })
                            cycle_begin = cycle_end
                            cycle_n_commits_snakemake_related = 0
                            cycle_n_commits = 0

                if "committer_epoch" in commit:
                    committer_epoch  = commit["committer_epoch"]
                    if new_repo_info["first_commit_at_epoch"] > committer_epoch:
                        new_repo_info["first_commit_at_epoch"] = committer_epoch

                    if new_repo_info["last_commit_at_epoch"] < committer_epoch:
                        new_repo_info["last_commit_at_epoch"] = committer_epoch

            new_repo_info["first_commit_at"] = util.epoch_to_str(
                epoch=new_repo_info["first_commit_at_epoch"],
                fmt="%Y-%m-%dT%H:%M:%S %z"
            )

            new_repo_info["last_commit_at"] = util.epoch_to_str(
                epoch=new_repo_info["last_commit_at_epoch"],
                fmt="%Y-%m-%dT%H:%M:%S %z"
            )

            # is still maintained?
            # no commit since in days

            new_repo_info["n_authors"] = len(authors)
            new_repo_info["n_committers"] = len(committers)

            # issues  = storage_handler.read(f"{METADATADIR}/{owner}_{repo_name}_issues.json", from_json=True)
            # events  = storage_handler.read(f"{METADATADIR}/{owner}_{repo_name}_events.json", from_json=True)
            # comments  = storage_handler.read(f"{METADATADIR}/{owner}_{repo_name}_comments.json", from_json=True)
            # pullrequests  = storage_handler.read(f"{METADATADIR}/{owner}_{repo_name}_pullrequests.json", from_json=True)

            storage_handler.write(f"{METADATADIR}/{owner}_{repo_name}_info.json", new_repo_info, to_json=True)
            printlog.info("Saved metadata for %s", repo_fullname)

        storage_handler.write(output[0], "")


rule extract_git_repositories_metadata:
    input:
        WORKDIR + "/.done_clone_repositories",
        WORKDIR + "/cloned_repositories.txt",
        WORKDIR + "/repositories_to_clone.txt",

    output:
        WORKDIR + "/.done_extract_git_repository_metadata",
        WORKDIR + "/extracted_commits_repositories.txt"

    run:
        cloned_repositories = storage_handler.read(input[1], multiple_lines=True)
        repositories_to_clone = storage_handler.read(input[2], multiple_lines=True)
        extracted_repositories = []

        for repo_fullname in repositories_to_clone:
            owner, repo_name = repo_fullname.strip().split("/")
            if repo_fullname not in cloned_repositories:
                printlog.warning("Repository not cloned: %s", repo_fullname)
                continue

            file_commit_metadata = f"{METADATADIR}/{owner}_{repo_name}_commits.json"

            if os.path.exists(file_commit_metadata):
                printlog.info("Metadata for commits is already saved for %s.", repo_fullname)
                extracted_repositories.append(repo_fullname)
                continue

            repo_path = os.path.join(WORKDIR, "cloned_repositories", repo_fullname.replace("/", "_"))

            gitrepo_handler = analysis_git.GitAnalysis(logger=logger, repo_path=repo_path)
            count_commits = gitrepo_handler.extract_commits(extend_for="snakemake")
            printlog.info("Extracted %d commits from %s", count_commits, repo_fullname)

            storage_handler.write(file_commit_metadata, gitrepo_handler.commits, to_json=True)
            extracted_repositories.append(repo_fullname)

        storage_handler.write(output[1], extracted_repositories)
        storage_handler.write(output[0], "")


rule get_issues_extras:
    input:
        WORKDIR + "/.done_get_issues",
        WORKDIR + "/repositories_with_issues.txt"

    output:
        WORKDIR + "/.done_get_issues_extras",

    run:
        repositories_with_issues = storage_handler.read(input[1], multiple_lines=True)

        for i, repo_fullname in enumerate(repositories_with_issues):
            owner, repo = repo_fullname.strip().split("/")
            all_issues           = None
            all_comments         = list()
            all_events           = list()
            issues_non_pr        = list()
            issues_as_pr         = list()
            issues_with_comments = list()
            all_pullrequests     = list()
            all_events           = list()
            file_issues          = f"{METADATADIR}/{owner}_{repo}_issues.json"
            file_comments        = f"{METADATADIR}/{owner}_{repo}_issues_comments.json"
            file_pullrequests    = f"{METADATADIR}/{owner}_{repo}_issues_pullrequests.json"
            file_events          = f"{METADATADIR}/{owner}_{repo}_issues_events.json"

            all_issues = storage_handler.read(file_issues, from_json=True)
            if all_issues is None:
                printlog.error("Failed to read %s.", file_issues)
                continue

            for issue in all_issues:
                if "comments" in issue and issue["comments"] > 0:
                    issues_with_comments.append(issue)

                if "pull_request" in issue:
                    issues_as_pr.append(issue)
                else:
                    issues_non_pr.append(issue)

            printlog.info("Getting extras from %d issues and %d pull requests from %s.",
                            len(issues_non_pr), len(issues_as_pr), repo_fullname)

            if os.path.exists(file_pullrequests):
                printlog.info("Pull requests are already saved for %s.", repo_fullname)
            else:
                for pullrequest in issues_as_pr:
                    issue_number = pullrequest["number"]
                    details = None
                    try:
                        details = gitapi_handler.get_pullrequest_details(owner, repo, issue_number)
                    except Exception as e:
                        printlog.error("Failed to get pull request details for %s/%s#%d: %s",
                                        owner, repo, issue_number, e)
                    if details is not None:
                        all_pullrequests.append(details)

                if len(all_pullrequests) > 0:
                    storage_handler.write(file_pullrequests, all_pullrequests, to_json=True)
                    printlog.info("Saved %d pull request details for %s",
                                    len(all_pullrequests), repo_fullname)

            if os.path.exists(file_comments):
                printlog.info("Comments are already saved for %s.", repo_fullname)
            else:
                for issue in issues_with_comments:
                    issue_number = issue["number"]
                    comments = None
                    try:
                        comments = gitapi_handler.get_issue_comments(owner, repo, issue_number)
                    except Exception as e:
                        printlog.error("Failed to get comments for %s/%s#%d: %s",
                                        owner, repo, issue_number, e)
                    if comments is not None and len(comments) > 0:
                        all_comments.extend(comments)
                    else:
                        printlog.warning("Failed to get comments for %s/%s#%d",
                                         owner, repo, issue_number)

                if len(all_comments) > 0:
                    storage_handler.write(file_comments, all_comments, to_json=True)
                    printlog.info("Saved %d comments of %d issues for %s",
                                    len(all_comments), len(issues_with_comments), repo_fullname)
                else:
                    printlog.debug("No comments found for %s", repo_fullname)

            if os.path.exists(file_events):
                printlog.info("Events are already saved for %s.", repo_fullname)
            else:
                for issue in all_issues:
                    issue_number = issue["number"]
                    events = None
                    try:
                        events = gitapi_handler.get_issue_events(owner, repo, issue_number)
                    except Exception as e:
                        printlog.error("Failed to get events for %s/%s#%d: %s",
                                        owner, repo, issue_number, e)
                    if events is not None and len(events) > 0:
                        all_events.extend(events)
                    else:
                        printlog.warning("Failed to get events for %s/%s#%d",
                                         owner, repo, issue_number)

                if len(all_events) > 0:
                    storage_handler.write(file_events, all_events, to_json=True)
                    printlog.info("Saved %d events of %d issues for %s",
                                    len(all_events), len(all_issues), repo_fullname)
                else:
                    printlog.debug("No events found for %s", repo_fullname)


        storage_handler.write(output[0], "")


rule get_issues:
    input:
        WORKDIR + "/.done_clone_repositories",
        WORKDIR + "/cloned_repositories.txt",
        WORKDIR + "/repositories_to_clone.txt"

    params:
        search_result_files = expand(WORKDIR + "/search_query_result_{start_date}_{end_date}.json", zip, start_date=START_DATES, end_date=END_DATES),

    output:
        WORKDIR + "/.done_get_issues",
        WORKDIR + "/repositories_with_issues.txt"

    run:
        cloned_repositories = []
        issues_repositories = []
        full_names = storage_handler.read(input[1], multiple_lines=True)
        for full_name in full_names:
            owner, repo = full_name.split("/")
            cloned_repositories.append({"owner": owner, "repo": repo})

        for search_result_file in params.search_result_files:
            search_api_response = storage_handler.read(search_result_file, from_json=True)
            if "items" not in search_api_response or len(search_api_response["items"]) < 1:
                printlog.error("No items in %s.", search_result_file)
                continue

            for repo_details in search_api_response["items"]:
                full_name = repo_details["full_name"]
                owner, repo = full_name.split("/")

                if os.path.exists(f"{METADATADIR}/{owner}_{repo}_issues.json"):
                    printlog.info("Issues are already saved for %s.", full_name)
                    issues_repositories.append(full_name)
                    continue

                if {"owner": owner, "repo": repo} not in cloned_repositories:
                    continue

                if "has_issues" not in repo_details or not repo_details["has_issues"]:
                    printlog.debug("Issues is disabled for %s", full_name)
                    continue

                issues = None
                try:
                    issues = gitapi_handler.get_issues(owner, repo)
                except Exception as e:
                    printlog.error("Failed to get issues for %s: %s", full_name, e)

                if issues is None or issues["total_count"] < 1:
                    printlog.debug("No issues in %s.", full_name)
                    continue

                storage_handler.write(f"{METADATADIR}/{owner}_{repo}_issues.json", issues["items"], to_json=True)
                printlog.info("Saved issues for %s, total: %d", full_name, issues["total_count"])
                issues_repositories.append(f"{owner}/{repo}")

        storage_handler.write(output[1], issues_repositories)
        storage_handler.write(output[0], "")


rule clone_repositories:
    input:
        WORKDIR + "/.done_download_skiplist_from_catalog",
        WORKDIR + "/skiplist_all.txt"

    params:
        repolist_file = WORKDIR + "/repository_list_from_search.txt"

    output:
        WORKDIR + "/cloned_repositories.txt",
        WORKDIR + "/repositories_to_clone.txt",
        WORKDIR + "/.done_clone_repositories"

    run:
        skiplist_all = storage_handler.read(input[1], multiple_lines=True)

        skip_count = 0
        target_count = 0
        repositories_to_clone = []
        searched_repositories = storage_handler.read(params.repolist_file, multiple_lines=True)
        for repo in searched_repositories:
            if repo in skiplist_all:
                skip_count += 1
            else:
                repositories_to_clone.append(repo)
                target_count += 1

        printlog.info("Number of repositories to clone: %s (skipped: %s)",
                                                target_count, skip_count)

        storage_handler.write(output[1], repositories_to_clone)

        cloned_repositories = gitapi_handler.clone_repositories(CLONEDIR,
                                repositories_to_clone)

        printlog.info("Number of cloned repositories: %s", len(cloned_repositories))

        storage_handler.write(output[0], cloned_repositories)
        storage_handler.write(output[2], "")


rule download_skiplist_from_catalog:
    input:
        WORKDIR + "/.done_search_repositories",

    params:
        file_skiplist  = WORKDIR + "/skips.json",
        file_blacklist = WORKDIR + "/blacklist.txt",

    output:
        WORKDIR + "/skips.json",
        WORKDIR + "/blacklist.txt",
        WORKDIR + "/skiplist_all.txt",
        WORKDIR + "/.done_download_skiplist_from_catalog"
    run:
        util.download_http_file(SKIPS, output[0])
        util.download_http_file(BLACKLIST, output[1])

        merged_skiplist = util.merge_skiplist(file_blacklist=output[1],
                          file_skiplist=output[0])

        storage_handler.write(output[2], merged_skiplist)
        storage_handler.write(output[3], "")


rule search_repositories:
    input:
        WORKDIR + "/search_queries.txt"

    params:
        json_files = expand(WORKDIR + "/search_query_result_{start_date}_{end_date}.json", zip, start_date=START_DATES, end_date=END_DATES)

    output:
        WORKDIR + "/.done_search_repositories",
        WORKDIR + "/repository_list_from_search.txt",
        expand(WORKDIR + "/search_query_result_{start_date}_{end_date}.json", zip, start_date=START_DATES, end_date=END_DATES)

    run:
        json_data = None

        query_strings = storage_handler.read(input[0], multiple_lines=True)
        for query_string in query_strings:
            date_range  = query_string.split("created:")[1].split("..")
            start_date  = date_range[0]
            end_date    = date_range[1]
            output_file = f"{WORKDIR}/search_query_result_{start_date}_{end_date}.json"

            # If the output file exists, this is a case that the rule was
            # stopped in the middle of the execution. Existing output file
            # means that the search for the query was already done.
            # Accordingly, we skip this query.
            if os.path.exists(output_file):
                continue

            result = gitapi_handler.search_repositories(query=query_string)
            storage_handler.write(output_file, result, to_json=True)

        items = []
        # create repository_list_from_search.txt
        for json_file in params.json_files:
            json_data = storage_handler.read(json_file, from_json=True)
            for item in json_data["items"]:
                items.append(item["full_name"])

        storage_handler.write(output[1], items)
        storage_handler.write(output[0], "")


rule generate_search_queries:
    input:
        WORKDIR + "/.initialized"
    output:
        WORKDIR + "/search_queries.txt"
    params:
        dates=search_date_ranges
    run:
        query_strings = []
        for date_range in params.dates:
            # snakemake workflow in:readme archived:false created:2024-11-01..2024-12-31
            query_string = f"{GIT_SEARCH_QUERY} created:{date_range[0]}..{date_range[1]}"
            query_strings.append(query_string)

        storage_handler.write(output[0], query_strings)

rule make_work_directory:
    """ Create a work directory as ./results/{GIT_PROVIDER}_{GIT_SEARCH_START_DATE}_{GIT_SEARCH_END_DATE} """
    output:
        WORKDIR + "/.initialized"
    run:
        os.makedirs(f"results/{GIT_PROVIDER}_{GIT_SEARCH_START_DATE}_{GIT_SEARCH_END_DATE}", exist_ok=True)
        # create .initialized file in the directory.
        storage_handler.write(output[0], "")
